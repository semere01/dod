{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import mahotas as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    textures = mt.features.haralick(image)\n",
    "    ht_mean = textures.mean(axis=0)\n",
    "    return ht_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_histogram(image, mask=None):\n",
    "    # convert the image to HSV color-shape\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # compute the color histogram\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    #return the histogram\n",
    "    return hist.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fd_hu_moments(image):\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# grab all image paths in the input dataset directory, then initialize\n",
    "# our list of images and corresponding class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = sorted(list(paths.list_images(\"dataset\")))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "global_feature=[]\n",
    "data = []\n",
    "labels = []\n",
    "IMAGE_DIMS = (50 ,50, 1)\n",
    "bins = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    fv_histogram = fd_histogram(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fv_hu_moments = fd_hu_moments(image)\n",
    "    features = extract_features(image)\n",
    "    global_feature = np.hstack([fv_histogram, features, fv_hu_moments])\n",
    "    data.append(global_feature)\n",
    "    l = label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dictionary of models our script can use, where the key \n",
    "# to the dictionary is the name of the model (supplied via command\n",
    "# line argument) and the value is the model itself\n",
    "models = {\n",
    "    \"knn\": KNeighborsClassifier(n_neighbors=1),\n",
    "    \"naive_bayes\": GaussianNB(),\n",
    "    \"logit\": LogisticRegression(solver=\"1bfgs\", multi_class=\"auto\"),\n",
    "    \"svm\":SVC(kernel=\"poly\", degree=2),\n",
    "    \"decision_tree\": DecisionTreeClassifier(),\n",
    "    \"random_forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"mlp\": MLPClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] constructing training/testing split...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# partition the data into training and testing splits, using 75%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# of the data for training and the remaining 25% for testing\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[INFO] constructing training/testing split...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m (trainData, testData, trainLabels, testLabels) \u001b[39m=\u001b[39m train_test_split(data, labels, test_size \u001b[39m=\u001b[39;49m \u001b[39m0.25\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2564\u001b[0m )\n\u001b[1;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2233\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2240\u001b[0m     )\n\u001b[1;32m   2242\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# partition the data into training and testing splits, using 75%\n",
    "# of the data for training and the remaining 25% for testing\n",
    "print(\"[INFO] constructing training/testing split...\")\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size = 0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
